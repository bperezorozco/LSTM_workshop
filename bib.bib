@BOOK{
         Bucker2005ADA,
       editor = "H. Martin B{\"u}cker and George F. Corliss and Paul D. Hovland and Uwe
         Naumann and Boyana Norris",
       title = "Automatic Differentiation: {A}pplications, Theory, and Implementations",
       series = "Lecture Notes in Computational Science and Engineering",
       publisher = "Springer",
       address = "New York, NY",
       ad_theotech = "General",
       year = "2005",
       volume = "50",
       doi = "10.1007/3-540-28438-9"
}
@article{Beaufays2014,
abstract = {Long Short-Term Memory (LSTM) is a specific recurrent neu- ral network (RNN) architecture thatwas designed to model tem- poral sequences and their long-range dependencies more accu- rately than conventional RNNs. In this paper, we exploreLSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic mod- eling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a lin- ear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effec- tive use of model parameters than the others considered, con- verges quickly, and outperforms a deep feed forward neural net- work having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Beaufays, Francoise and Sak, Hasim and Senior, Andrew},
eprint = {arXiv:1402.1128v1},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/43905.pdf:pdf},
journal = {Interspeech},
mendeley-groups = {LSTMWorkshop},
number = {September},
pages = {338--342},
title = {{Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling Has}},
year = {2014}
}
@article{Graves2013_2,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1308.0850v5.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint arXiv:1308.0850},
mendeley-groups = {LSTMWorkshop},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:home/ber/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves, Mohamed, Hinton - 2013 - Speech Recognition With Deep Recurrent Neural Networks.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Icassp},
mendeley-groups = {RNN,LSTMWorkshop},
title = {{Speech Recognition With Deep Recurrent Neural Networks}},
year = {2013}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1412.6980v8.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {LSTMWorkshop},
pages = {1--13},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Le2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209v5},
author = {Le, Quoc V and Ranzato, Marc Aurelio and Devin, Matthieu and Corrado, Greg S and Ng, Andrew Y},
eprint = {arXiv:1112.6209v5},
file = {:home/ber/Dropbox/to{\_}read/high{\_}level{\_}features{\_}deep{\_}learning.pdf:pdf},
mendeley-groups = {Machine Learning,DLPrimer,LSTMWorkshop},
title = {{Building High-level Features Using Large Scale Unsupervised Learning}},
year = {2012}
}
@article{Long2014,
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convo-lutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative con-vnet model is trained using the Generative Adversarial Nets (GAN) approach [10]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 sam-ples were mistaken for real images around 40{\%} of the time, compared to 10{\%} for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
archivePrefix = {arXiv},
arxivId = {1411.5908},
author = {Long, Jonathan and Shelhamer, Evan and Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru and Lenc, Karel and Vedaldi, Andrea and Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob and Fischer, Philipp and Philip, H and Hazırbas, Caner and Smagt, Patrick Van Der and Cremers, Daniel and Brox, Thomas and Meng, Fandong and Lu, Zhengdong and Tu, Zhaopeng and Li, Hang and Liu, Qun and Mahadevan, Vijay and Member, Student},
eprint = {1411.5908},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1411.4555v2.pdf:pdf},
journal = {arXiv},
mendeley-groups = {LSTMWorkshop},
number = {1},
pages = {1--10},
title = {{Show and Tell: A Neural Image Caption Generator}},
url = {http://arxiv.org/abs/1411.5908v1},
volume = {32},
year = {2014}
}
@article{Prasad2014,
abstract = {Ability of deep networks to extract high level features and of recurrent networks to perform time-series inference have been studied. In view of universality of one hidden layer network at approximating functions under weak constraints, the benefit of multiple layers is to enlarge the space of dynamical systems approximated or, given the space, reduce the number of units required for a certain error. Traditionally shallow networks with manually engineered features are used, back-propagation extent is limited to one and attempt to choose a large number of hidden units to satisfy the Markov condition is made. In case of Markov models, it has been shown that many systems need to be modeled as higher order. In the present work, we present deep recurrent networks with longer backpropagation through time extent as a solution to modeling systems that are high order and to predicting ahead. We study epileptic seizure suppression electro-stimulator. Extraction of manually engineered complex features and prediction employing them has not allowed small low-power implementations as, to avoid possibility of surgery, extraction of any features that may be required has to be included. In this solution, a recurrent neural network performs both feature extraction and prediction. We prove analytically that adding hidden layers or increasing backpropagation extent increases the rate of decrease of approximation error. A Dynamic Programming (DP) training procedure employing matrix operations is derived. DP and use of matrix operations makes the procedure efficient particularly when using data-parallel computing. The simulation studies show the geometry of the parameter space, that the network learns the temporal structure, that parameters converge while model output displays same dynamic behavior as the system and greater than .99 Average Detection Rate on all real seizure data tried.},
archivePrefix = {arXiv},
arxivId = {1407.5949},
author = {Prasad, Sharat C and Prasad, Piyush},
eprint = {1407.5949},
file = {:home/ber/Dropbox/to{\_}read/tsprediction{\_}drnn.pdf:pdf},
mendeley-groups = {DLPrimer,LSTMWorkshop},
pages = {1--54},
title = {{Deep Recurrent Neural Networks for Time Series Prediction}},
url = {http://arxiv.org/abs/1407.5949},
volume = {95070},
year = {2014}
}
@article{Simonyan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1409.1556.pdf:pdf},
mendeley-groups = {LSTMWorkshop},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2015}
}
@article{Szegedy,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.4842v1},
author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {arXiv:1409.4842v1},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1409.4842v1.pdf:pdf},
mendeley-groups = {LSTMWorkshop},
pages = {1--12},
title = {{Going deeper with convolutions}}
}
@article{You2016,
abstract = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1603.03925},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
doi = {10.1109/CVPR.2016.503},
eprint = {1603.03925},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/362{\_}Report.pdf:pdf},
journal = {Cvpr},
mendeley-groups = {LSTMWorkshop},
number = {1},
pages = {10},
title = {{Image Captioning with Semantic Attention}},
url = {http://arxiv.org/abs/1603.03925},
year = {2016}
}
@article{Cybenko1993,
abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
author = {Cybenko, G.},
doi = {10.1007/BF02836480},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/Cybenko{\_}MCSS.pdf:pdf},
isbn = {0780300564},
issn = {10009221},
journal = {Approximation Theory and its Applications},
keywords = {approximation,completeness,neural networks},
mendeley-groups = {LSTMWorkshop},
number = {3},
pages = {17--28},
title = {{Degree of approximation by superpositions of a sigmoidal function}},
volume = {9},
year = {1993}
}
@article{Paul2014,
abstract = {Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called pre-training: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of {\{}$\backslash$em shadow{\}} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the {\{}$\backslash$em simplest{\}}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.},
archivePrefix = {arXiv},
arxivId = {1412.6621},
author = {Paul, Arnab and Venkatasubramanian, Suresh},
eprint = {1412.6621},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1412.6621v3.pdf:pdf},
isbn = {1412.6621},
journal = {Arxiv},
mendeley-groups = {LSTMWorkshop},
pages = {13},
title = {{Why does Deep Learning work? - A perspective from Group Theory}},
url = {http://arxiv.org/abs/1412.6621},
year = {2014}
}
@article{Schmidhuber2007,
abstract = {In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM.},
author = {Schmidhuber, J{\"{u}}rgen and Wierstra, Daan and Gagliolo, Matteo and Gomez, Faustino},
doi = {10.1162/neco.2007.19.3.757},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/nc2007.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {evolino,ining recurrent networks by},
mendeley-groups = {LSTMWorkshop},
number = {3},
pages = {757--779},
pmid = {17298232},
title = {{Training recurrent networks by Evolino.}},
volume = {19},
year = {2007}
}
@article{Sexton1999,
abstract = {The escalation of Neural Network research in Business has been brought about by the ability of neural networks, as a tool, to closely approximate unknown functions to any degree of desired accuracy. Although, gradient based search techniques such as back-propagation are currently the most widely used optimization techniques for training neural networks, it has been shown that these gradient techniques are severely limited in their ability to find global solutions. Global search techniques have been identified as a potential solution to this problem. In this paper we examine two well known global search techniques, Simulated Annealing and the Genetic Algorithm, and compare their performance. A Monte Carlo study was conducted in order to test the appropriateness of these global search techniques for optimizing neural networks.},
author = {Sexton, Randall S. and Dorsey, Robert E. and Johnson, John D.},
doi = {10.1016/S0377-2217(98)00114-3},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/gasa.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
mendeley-groups = {LSTMWorkshop},
number = {3},
pages = {589--601},
title = {{Optimization of neural networks: A comparative analysis of the genetic algorithm and simulated annealing}},
volume = {114},
year = {1999}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/nnlstmkalman.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
mendeley-groups = {LSTMWorkshop},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets}},
volume = {53},
year = {2013}
}
@article{DeFreitas1999,
abstract = {The application of the Bayesian learning paradigm to neural networks results in a flexi- ble and powerful nonlinear modelling framework that can be used for regression, den- sity estimation, prediction and classification. Within this framework, all sources of uncertainty ...},
author = {{De Freitas}, J F G},
doi = {10.1214/088342304000000099},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/thesis.pdf:pdf},
journal = {Unpublished doctoral dissertation},
mendeley-groups = {LSTMWorkshop},
title = {{Bayesian methods for neural networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.8764{\&}rep=rep1{\&}type=pdf$\backslash$npapers2://publication/uuid/7EC44251-5C0E-4C76-8156-FE36002D1960},
year = {1999}
}
@misc{Ruder2015,
author = {Ruder, Sebastian},
mendeley-groups = {LSTMWorkshop},
title = {{An overview of gradient descent optimisation algorithms}},
year = {2015}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/duchi11a.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
mendeley-groups = {LSTMWorkshop},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/pascanu13.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
mendeley-groups = {LSTMWorkshop},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Gers2002,
abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
author = {Gers, Felix a and Schraudolph, Nicol N and Schmidhuber, Jurgen},
doi = {10.1162/153244303768966139},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/gers02a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {long short term memory,recurrent neural networks,timing},
mendeley-groups = {LSTMWorkshop},
number = {1},
pages = {115--143},
pmid = {17272722},
title = {{Learning Precise Timing with LSTM Recurrent Networks}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2002}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Hochreiter, S and Schmidhuber, J{\"{u}}rgen and Schmidhuber, J},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/Hochreiter97{\_}lstm.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Memory,Memory, Short-Term,Models,Models, Neurological,Models, Psychological,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurological,Psychological,Short-Term,Time Factors},
mendeley-groups = {LSTMWorkshop},
number = {8},
pages = {1735--80},
pmid = {9377276},
title = {{Long short-term memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9377276},
volume = {9},
year = {1997}
}
@article{Bottou2010,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, Le{\'{o}}n},
doi = {10.1007/978-3-7908-2604-3_16},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/compstat-2010.pdf:pdf},
isbn = {0269-2155},
issn = {0269-2155},
journal = {Proceedings of COMPSTAT'2010},
keywords = {efficiency,online learning,stochastic gradient descent},
mendeley-groups = {LSTMWorkshop},
pages = {177--186},
pmid = {20876631},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
year = {2010}
}
@article{Rutkauskas2011,
author = {Rutkauskas, Aleksandras Vytautas and Maknickas, Algirdas and Maknickienė, N},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/Maknickiene-3-8-Pages-from-IITSBE-2011-2-Nr11.pdf:pdf},
journal = {Innovative Infotechnologies for Science, Business and Education},
keywords = {dynamic quantile regressions,dynamic treatment models,evolino learning algorithm,financial forecasting and simulation,forecasting models,neural networks and related,non linear time series,orthogonal inputs,pics,prediction of financial markets,recurrent neural networks,simulation methods,time-series models,to-},
mendeley-groups = {LSTMWorkshop},
number = {687},
pages = {3--8},
title = {{Investigation of financial market prediction by recurrent neural network}},
volume = {2},
year = {2011}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these con-trollers have limited memory and rely on being able to perceive the complete game screen at each deci-sion point. To address these shortcomings, this arti-cle investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recur-rent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with in-crementally more complete observations, DRQN's per-formance scales as a function of observability. Con-versely, when trained with full observations and eval-uated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learn-ing to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1507.06527.pdf:pdf},
isbn = {9781577357520},
journal = {arXiv preprint arXiv:1507.06527},
mendeley-groups = {LSTMWorkshop},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
year = {2015}
}
@misc{Karpathy2015,
author = {Karpathy, Andrej},
mendeley-groups = {LSTMWorkshop},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
year = {2015}
}
@article{Karpathy2016,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1506.02078v1},
file = {:home/ber/Documents/dphil/lstm{\_}workshop/lit/1506.02078.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Iclr},
mendeley-groups = {LSTMWorkshop},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}},
year = {2016}
}

